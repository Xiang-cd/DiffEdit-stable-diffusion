{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c14590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "from numpy import asarray\n",
    "import random\n",
    "from PIL import Image\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.models.diffusion.dpm_solver import DPMSolverSampler\n",
    "from ldm.models.diffusion.dpm_solver import (\n",
    "    model_wrapper,\n",
    "    NoiseScheduleVP,\n",
    "    DPM_Solver\n",
    ")\n",
    "\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--prompt\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=\"a painting of a virus monster playing guitar\",\n",
    "        help=\"the prompt to render\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--prompt_edit\",\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        default=None,\n",
    "        help=\"the prompt to be edit\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--ddim_steps\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"number of ddim sampling steps\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--H\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"image height, in pixel space\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--W\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"image width, in pixel space\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_samples\",\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help=\"how many samples to produce for each given prompt. A.k.a. batch size\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--scale\",\n",
    "        type=float,\n",
    "        default=7.5,\n",
    "        help=\"unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        type=str,\n",
    "        default=\"configs/stable-diffusion/v1-inference.yaml\",\n",
    "        help=\"path to config which constructs model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ckpt\",\n",
    "        type=str,\n",
    "        default=\"models/ldm/stable-diffusion-v1/model.ckpt\",\n",
    "        help=\"path to checkpoint of model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ddim_eta\",\n",
    "        type=float,\n",
    "        default=0.0,\n",
    "        help=\"ddim eta (eta=0.0 corresponds to deterministic sampling\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        default=42,\n",
    "        help=\"the seed (for reproducible sampling)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--precision\",\n",
    "        type=str,\n",
    "        help=\"evaluate at this precision\",\n",
    "        choices=[\"full\", \"autocast\"],\n",
    "        default=\"autocast\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--origin_image\",\n",
    "        type=str,\n",
    "        help=\"the path of image to be edit\",\n",
    "        default=None,\n",
    "        required=True\n",
    "    )\n",
    "\n",
    "    # opt = parser.parse_args()\n",
    "    opt = parser.parse_args(args=[\n",
    "        \"--ddim_eta\", \"0.0\",\n",
    "        \"--n_samples\", \"1\",\n",
    "        \"--scale\", \"10.0\",\n",
    "        \"--ddim_steps\", \"20\",\n",
    "        \"--seed\", \"42\",\n",
    "        \"--ckpt\", \"your ckpt path\",\n",
    "        \"--prompt\", \"photo of a sks dog\",\n",
    "        \"--origin_image\", \"data/fruit.png\"\n",
    "    ])\n",
    "\n",
    "    config = OmegaConf.load(f\"{opt.config}\")\n",
    "    model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    return model, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ca73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path, opt):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    #     w, h = image.size\n",
    "    #     print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "    #     w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((opt.W, opt.H), resample=Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2. * image - 1.\n",
    "\n",
    "\n",
    "def latent_to_image(model, latents):\n",
    "    x_samples = model.decode_first_stage(latents)\n",
    "    x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "    x_samples = x_samples.cpu().permute(0, 2, 3, 1).numpy()\n",
    "    x_samples = 255. * x_samples\n",
    "    x_samples = x_samples.astype(np.uint8)\n",
    "\n",
    "    return x_samples\n",
    "\n",
    "\n",
    "def repeat_tensor(x, n, dim=0):\n",
    "    dims = len(x.shape) * [1]\n",
    "    dims[dim] = n\n",
    "    return x.repeat(dims)\n",
    "\n",
    "\n",
    "def get_mask(model, src, dst, init_latent, n: int, ddim_steps,\n",
    "             clamp_rate:float=4):\n",
    "    \"\"\"\n",
    "    the map value will be clamped to map.mean() * clamp_rate, then values will be scaled into 0~1, then term into binary(split at 0.5). so if a map value is large than map.mean() * clamp_rate * 0.5 will be encode to 1, less will be encode to 0. \n",
    "    so the larger clamp rate is, less pixes will be encode to 1, the small clamp rate is, the more pixes will be encode to 1.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    repeated = repeat_tensor(init_latent, n)\n",
    "    src = repeat_tensor(src, n)\n",
    "    dst = repeat_tensor(dst, n)\n",
    "    noise = torch.randn(init_latent.shape, device=device)\n",
    "    scheduler = DDIMScheduler(num_train_timesteps=model.num_timesteps,\n",
    "                              trained_betas=model.betas.cpu().numpy())\n",
    "    scheduler.set_timesteps(ddim_steps, device=device)\n",
    "    noised = scheduler.add_noise(repeated, noise,\n",
    "                                 scheduler.timesteps[ddim_steps // 2]\n",
    "                                 )\n",
    "\n",
    "    t = scheduler.timesteps[ddim_steps // 2]\n",
    "    t_ = torch.unsqueeze(t, dim=0).to(device)\n",
    "    pre_src = model.apply_model(noised, t_, src)\n",
    "    pre_dst = model.apply_model(noised, t_, dst)\n",
    "\n",
    "    # consider to add smooth method\n",
    "    subed = (pre_src - pre_dst).abs_().mean(dim=[0, 1])\n",
    "    max_v = subed.mean() * clamp_rate\n",
    "    mask = subed.clamp(0, max_v) / max_v\n",
    "\n",
    "    def to_binary(pix):\n",
    "        if pix > 0.5:\n",
    "            return 1.\n",
    "        else:\n",
    "            return 0.\n",
    "\n",
    "    mask = mask.cpu().apply_(to_binary).to(device)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdacdf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify sample method of dpm_solver, most code are copied from https://github.com/LuChengTHU/dpm-solver/blob/main/dpm_solver_pytorch.py\n",
    "# here we just record sample lantents and apply mask in sample process\n",
    "def sample_edit(self, x, steps=20, t_start=None, t_end=None, order=3, skip_type='time_uniform',\n",
    "                method='singlestep', lower_order_final=True, denoise_to_zero=False, solver_type='dpm_solver',\n",
    "                atol=0.0078, rtol=0.05, record_list=None, mask=None\n",
    "                ):\n",
    "    t_0 = 1. / self.noise_schedule.total_N if t_end is None else t_end\n",
    "    t_T = self.noise_schedule.T if t_start is None else t_start\n",
    "    device = x.device\n",
    "    if record_list is not None:\n",
    "        assert len(record_list) == steps\n",
    "    if method == 'adaptive':\n",
    "        with torch.no_grad():\n",
    "            x = self.dpm_solver_adaptive(x, order=order, t_T=t_T, t_0=t_0, atol=atol, rtol=rtol,\n",
    "                                         solver_type=solver_type)\n",
    "    elif method == 'multistep':\n",
    "        assert steps >= order\n",
    "        timesteps = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=steps, device=device)\n",
    "        assert timesteps.shape[0] - 1 == steps\n",
    "        with torch.no_grad():\n",
    "            vec_t = timesteps[0].expand((x.shape[0]))\n",
    "            model_prev_list = [self.model_fn(x, vec_t)]\n",
    "            t_prev_list = [vec_t]\n",
    "            # Init the first `order` values by lower order multistep DPM-Solver.\n",
    "            for init_order in range(1, order):\n",
    "                vec_t = timesteps[init_order].expand(x.shape[0])\n",
    "                x = self.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, vec_t, init_order,\n",
    "                                                     solver_type=solver_type)\n",
    "                if mask is not None and record_list is not None:\n",
    "                    x = record_list[init_order - 1].to(device) * (1. - mask) + x * mask\n",
    "                model_prev_list.append(self.model_fn(x, vec_t))\n",
    "                t_prev_list.append(vec_t)\n",
    "            # Compute the remaining values by `order`-th order multistep DPM-Solver.\n",
    "            for step in range(order, steps + 1):\n",
    "                vec_t = timesteps[step].expand(x.shape[0])\n",
    "                if lower_order_final and steps < 15:\n",
    "                    step_order = min(order, steps + 1 - step)\n",
    "                else:\n",
    "                    step_order = order\n",
    "                x = self.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, vec_t, step_order,\n",
    "                                                     solver_type=solver_type)\n",
    "                if mask is not None and record_list is not None:\n",
    "                    x = record_list[step - 1].to(device) * (1. - mask) + x * mask\n",
    "                for i in range(order - 1):\n",
    "                    t_prev_list[i] = t_prev_list[i + 1]\n",
    "                    model_prev_list[i] = model_prev_list[i + 1]\n",
    "                t_prev_list[-1] = vec_t\n",
    "                # We do not need to evaluate the final model value.\n",
    "                if step < steps:\n",
    "                    model_prev_list[-1] = self.model_fn(x, vec_t)\n",
    "    elif method in ['singlestep', 'singlestep_fixed']:\n",
    "        if method == 'singlestep':\n",
    "            timesteps_outer, orders = self.get_orders_and_timesteps_for_singlestep_solver(steps=steps, order=order,\n",
    "                                                                                          skip_type=skip_type, t_T=t_T,\n",
    "                                                                                          t_0=t_0, device=device)\n",
    "        elif method == 'singlestep_fixed':\n",
    "            K = steps // order\n",
    "            orders = [order, ] * K\n",
    "            timesteps_outer = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=K, device=device)\n",
    "        for i, order in enumerate(orders):\n",
    "            t_T_inner, t_0_inner = timesteps_outer[i], timesteps_outer[i + 1]\n",
    "            timesteps_inner = self.get_time_steps(skip_type=skip_type, t_T=t_T_inner.item(), t_0=t_0_inner.item(),\n",
    "                                                  N=order, device=device)\n",
    "            lambda_inner = self.noise_schedule.marginal_lambda(timesteps_inner)\n",
    "            vec_s, vec_t = t_T_inner.tile(x.shape[0]), t_0_inner.tile(x.shape[0])\n",
    "            h = lambda_inner[-1] - lambda_inner[0]\n",
    "            r1 = None if order <= 1 else (lambda_inner[1] - lambda_inner[0]) / h\n",
    "            r2 = None if order <= 2 else (lambda_inner[2] - lambda_inner[0]) / h\n",
    "            x = self.singlestep_dpm_solver_update(x, vec_s, vec_t, order, solver_type=solver_type, r1=r1, r2=r2)\n",
    "    if denoise_to_zero:\n",
    "        x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def sample(self, x, steps=20, t_start=None, t_end=None, order=3, skip_type='time_uniform',\n",
    "           method='singlestep', lower_order_final=True, denoise_to_zero=False, solver_type='dpm_solver',\n",
    "           atol=0.0078, rtol=0.05, record_process=False, record_list=None\n",
    "           ):\n",
    "    t_0 = 1. / self.noise_schedule.total_N if t_end is None else t_end\n",
    "    t_T = self.noise_schedule.T if t_start is None else t_start\n",
    "    device = x.device\n",
    "    if method == 'adaptive':\n",
    "        with torch.no_grad():\n",
    "            x = self.dpm_solver_adaptive(x, order=order, t_T=t_T, t_0=t_0, atol=atol, rtol=rtol,\n",
    "                                         solver_type=solver_type)\n",
    "    elif method == 'multistep':\n",
    "        assert steps >= order\n",
    "        timesteps = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=steps, device=device)\n",
    "        assert timesteps.shape[0] - 1 == steps\n",
    "        with torch.no_grad():\n",
    "            vec_t = timesteps[0].expand((x.shape[0]))\n",
    "            model_prev_list = [self.model_fn(x, vec_t)]\n",
    "            t_prev_list = [vec_t]\n",
    "            # Init the first `order` values by lower order multistep DPM-Solver.\n",
    "            for init_order in range(1, order):\n",
    "                vec_t = timesteps[init_order].expand(x.shape[0])\n",
    "                x = self.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, vec_t, init_order,\n",
    "                                                     solver_type=solver_type)\n",
    "                if record_process:\n",
    "                    record_list.append(x.cpu())\n",
    "                model_prev_list.append(self.model_fn(x, vec_t))\n",
    "                t_prev_list.append(vec_t)\n",
    "            # Compute the remaining values by `order`-th order multistep DPM-Solver.\n",
    "            for step in range(order, steps + 1):\n",
    "                vec_t = timesteps[step].expand(x.shape[0])\n",
    "                if lower_order_final and steps < 15:\n",
    "                    step_order = min(order, steps + 1 - step)\n",
    "                else:\n",
    "                    step_order = order\n",
    "                x = self.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, vec_t, step_order,\n",
    "                                                     solver_type=solver_type)\n",
    "                if record_process:\n",
    "                    record_list.append(x.cpu())\n",
    "                for i in range(order - 1):\n",
    "                    t_prev_list[i] = t_prev_list[i + 1]\n",
    "                    model_prev_list[i] = model_prev_list[i + 1]\n",
    "                t_prev_list[-1] = vec_t\n",
    "                # We do not need to evaluate the final model value.\n",
    "                if step < steps:\n",
    "                    model_prev_list[-1] = self.model_fn(x, vec_t)\n",
    "    elif method in ['singlestep', 'singlestep_fixed']:\n",
    "        if method == 'singlestep':\n",
    "            timesteps_outer, orders = self.get_orders_and_timesteps_for_singlestep_solver(steps=steps, order=order,\n",
    "                                                                                          skip_type=skip_type, t_T=t_T,\n",
    "                                                                                          t_0=t_0, device=device)\n",
    "        elif method == 'singlestep_fixed':\n",
    "            K = steps // order\n",
    "            orders = [order, ] * K\n",
    "            timesteps_outer = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=K, device=device)\n",
    "        for i, order in enumerate(orders):\n",
    "            t_T_inner, t_0_inner = timesteps_outer[i], timesteps_outer[i + 1]\n",
    "            timesteps_inner = self.get_time_steps(skip_type=skip_type, t_T=t_T_inner.item(), t_0=t_0_inner.item(),\n",
    "                                                  N=order, device=device)\n",
    "            lambda_inner = self.noise_schedule.marginal_lambda(timesteps_inner)\n",
    "            vec_s, vec_t = t_T_inner.tile(x.shape[0]), t_0_inner.tile(x.shape[0])\n",
    "            h = lambda_inner[-1] - lambda_inner[0]\n",
    "            r1 = None if order <= 1 else (lambda_inner[1] - lambda_inner[0]) / h\n",
    "            r2 = None if order <= 2 else (lambda_inner[2] - lambda_inner[0]) / h\n",
    "            x = self.singlestep_dpm_solver_update(x, vec_s, vec_t, order, solver_type=solver_type, r1=r1, r2=r2)\n",
    "    if denoise_to_zero:\n",
    "        x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90010056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffedit(model, init_image,\n",
    "             src_prompt: str = \"A bowl of fruits\",\n",
    "             dst_prompt: str = \"A bowl of pears\",\n",
    "             encode_ratio: float = 0.6,\n",
    "             ddim_steps: int = 15,\n",
    "             seed: int = 42,\n",
    "             scale: float = 7.5,\n",
    "             precision=\"autocast\"):\n",
    "    \"\"\"\n",
    "    :param init_image: image to be edit\n",
    "    :param src_prompt: prompt describe origin image(i.e. A bowl of fruits)\n",
    "    :param dst_prompt: prompt describe desired image(i.e. A bowl of pears)\n",
    "    :param encode_ratio: how deep to encode origin image, must between 0-1\n",
    "    :param ddim_steps: total ddim steps, actual encode steps = ddim_steps * encode ratio\n",
    "    :param seed: random seed\n",
    "    :param scale: classifier free guidance scale\n",
    "    :param precision: ema precision\n",
    "    \"\"\"\n",
    "    #If seed is None, randomly select seed from 0 to 2^32-1\n",
    "    if seed is None:\n",
    "        seed = random.randrange(2 ** 32 - 1)\n",
    "    seed_everything(seed)\n",
    "    device = model.device\n",
    "\n",
    "    model.cond_stage_model = model.cond_stage_model.to(device)\n",
    "    precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "    assert os.path.isfile(opt.origin_image)\n",
    "    init_image = load_img(opt.origin_image, opt).to(device)\n",
    "    init_image = repeat(init_image, '1 ... -> b ...', b=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(device.type):\n",
    "            with model.ema_scope():\n",
    "                uc = None\n",
    "                if scale != 1.0:\n",
    "                    uc = model.get_learned_conditioning([\"\"])\n",
    "                src = model.get_learned_conditioning([src_prompt])\n",
    "                dst = model.get_learned_conditioning([dst_prompt])\n",
    "                init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))\n",
    "\n",
    "                # get mask\n",
    "                mask = get_mask(model, src, dst, init_latent, 3, ddim_steps)\n",
    "                plt.imshow(mask.detach().cpu().numpy())\n",
    "                plt.show()\n",
    "                \n",
    "                ns = NoiseScheduleVP('discrete', betas=model.betas)\n",
    "                model_fn = model_wrapper(\n",
    "                    lambda x, t, c: model.apply_model(x, t, c),\n",
    "                    ns,\n",
    "                    model_type=\"noise\",\n",
    "                    guidance_type=\"classifier-free\",\n",
    "                    condition=src,\n",
    "                    unconditional_condition=uc,\n",
    "                    guidance_scale=scale\n",
    "                )\n",
    "\n",
    "                # add noise and record each step's output latent\n",
    "                noiser = DPM_Solver(model_fn, ns, predict_x0=True, thresholding=False)\n",
    "                noiser.sample = sample.__get__(noiser, type(noiser))\n",
    "                record_list = []\n",
    "                noised_sample = noiser.sample(\n",
    "                    init_latent,\n",
    "                    t_start=1. / model.num_timesteps,\n",
    "                    t_end=encode_ratio,\n",
    "                    method='multistep',\n",
    "                    order=3,\n",
    "                    steps=ddim_steps,\n",
    "                    record_process=True,\n",
    "                    record_list=record_list\n",
    "                )\n",
    "\n",
    "                # perform step wise edit\n",
    "                model_fn_dst = model_wrapper(\n",
    "                    lambda x, t, c: model.apply_model(x, t, c),\n",
    "                    ns,\n",
    "                    model_type=\"noise\",\n",
    "                    guidance_type=\"classifier-free\",\n",
    "                    condition=dst,\n",
    "                    unconditional_condition=uc,\n",
    "                    guidance_scale=scale\n",
    "                )\n",
    "                solver = DPM_Solver(model_fn_dst, ns, predict_x0=True, thresholding=False)\n",
    "\n",
    "                solver.sample_edit = sample_edit.__get__(solver, type(solver))\n",
    "                recover = solver.sample_edit(\n",
    "                    noised_sample,\n",
    "                    t_start=encode_ratio,\n",
    "                    t_end=1. / model.num_timesteps,\n",
    "                    method='multistep',\n",
    "                    order=3,\n",
    "                    steps=ddim_steps,\n",
    "                    mask=mask,\n",
    "                    record_list=list(reversed(record_list))\n",
    "                )\n",
    "\n",
    "                images = latent_to_image(model, recover)\n",
    "                return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ed09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = diffedit(model, \"data/fruit.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(res[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
